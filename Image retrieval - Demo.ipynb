{"cells":[{"cell_type":"markdown","metadata":{"id":"ytB37MWwq5K4"},"source":["# Preliminary operations"]},{"cell_type":"markdown","metadata":{"id":"5Wbyemljr_qB"},"source":["We import libraries and datasets."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yu3Mcu_Tq23l","executionInfo":{"status":"ok","timestamp":1675287438200,"user_tz":-60,"elapsed":547,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"}}},"outputs":[],"source":["# Utility\n","from google.colab import drive\n","from shutil import copyfile\n","import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import numpy as np\n","import subprocess\n","import tkinter as tk\n","import random\n","\n","# Image processing\n","import cv2\n","from PIL import Image\n","\n","# KDTree\n","from sklearn.neighbors import KDTree\n","import joblib\n","\n","# Keras\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing import image as kimage\n","from tensorflow.keras.applications import mobilenet_v2"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B6FZ9uhfybKF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675287009872,"user_tz":-60,"elapsed":29606,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"}},"outputId":"a891d3f5-051e-4926-f00b-18304004380d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["Loading the tree."],"metadata":{"id":"7_vUMlK13-JK"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"yvy_1JdTg8Bx","executionInfo":{"status":"ok","timestamp":1675287037828,"user_tz":-60,"elapsed":24801,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"}}},"outputs":[],"source":["tree = joblib.load('/content/gdrive/MyDrive/Digital Signal/Models/Image Retrieval/mob_tree.joblib')"]},{"cell_type":"markdown","source":["Loading the MobileNetV2."],"metadata":{"id":"hOfUPR_i68yS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJ3r4UFVrsnN"},"outputs":[],"source":["mobilenet = keras.applications.MobileNetV2(input_shape = (224, 224, 3), weights = 'imagenet', include_top = False, pooling = 'max')"]},{"cell_type":"markdown","source":["Loading the dataframes for computing accuracy."],"metadata":{"id":"n96rMlZN7JQd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8IP2P6HKxxu"},"outputs":[],"source":["df_actors = pd.read_csv('/content/gdrive/MyDrive/Digital Signal/Dataset/list_attr_actors.csv')\n","df_celeba = pd.read_csv('/content/gdrive/MyDrive/Digital Signal/Dataset/list_attr_celeba.csv')"]},{"cell_type":"markdown","metadata":{"id":"L3EwsRlA2vaa"},"source":["# Acquisition"]},{"cell_type":"code","source":["root= tk.Tk()\n","\n","canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n","canvas1.pack()\n","\n","label1 = tk.Label(root, text='Select clip to analize')\n","label1.config(font=('helvetica', 16))\n","canvas1.create_window(200, 25, window=label1)\n","\n","label2 = tk.Label(root, text='Number from 0 to 6:')\n","label2.config(font=('helvetica', 11))\n","canvas1.create_window(200, 100, window=label2)\n","\n","def display_text():\n","   global example\n","   example = int(example.get())\n","   root.destroy\n","\n","example = tk.Entry(root)\n","example.pack()\n","canvas1.create_window(200, 140, window=example)\n","\n","    \n","button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n","canvas1.create_window(200, 180, window=button1)\n","\n","root.mainloop()"],"metadata":{"id":"DZ_caZGo2uY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fn = os.listdir('/content/gdrive/MyDrive/Digital Signal/Dataset/Actor_faces/')\n","actor_path = '/content/gdrive/MyDrive/Digital Signal/Dataset/Actor_faces/' + fn[example]"],"metadata":{"id":"ls9iaqU94-Hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#player = subprocess.call([path per lettore immagini, actor_path, '--play-and-exit'])"],"metadata":{"id":"NhKSMtrI5hGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Processing"],"metadata":{"id":"c4Gqu4tw2uoE"}},{"cell_type":"markdown","metadata":{"id":"-WP0oaTebgpW"},"source":["In order to crop the background as much as possible, we use a Haar cascade classifier to detect and crop faces in both datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJ6G5veEb-1s"},"outputs":[],"source":["# Load the cascade classifier\n","face_cascade = cv2.CascadeClassifier('/content/gdrive/MyDrive/Digital Signal/Dataset/haarcascade_frontalface_default.xml')\n","\n","def crop_face(path, scaleFactor, minNeighbors):\n","  # Read the input image\n","  im = cv2.imread(path)\n","  # Convert the image to grayscale\n","  gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n","  # Detect faces in the image\n","  faces = face_cascade.detectMultiScale(gray, scaleFactor, minNeighbors)\n","  if len(faces) == 0:\n","    # We may try to decrease the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor - 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        a, b, c = 0, 0, 0\n","        # Crop the face\n","        if(y-30 >= 0):\n","          a = 30\n","        if(y+h+10 < 218):\n","          b = 10\n","        if(x-5 >= 0 and x+w+5 < 178):\n","          c = 5\n","        face = im[y-a:y+h+b, x-c:x+w+c]\n","        return (2, face)\n","  elif len(faces) > 1:\n","    # We may try to increase the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor + 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        a, b, c = 0, 0, 0\n","        # Crop the face\n","        if(y-30 >= 0):\n","          a = 30\n","        if(y+h+10 < 218):\n","          b = 10\n","        if(x-5 >= 0 and x+w+5 < 178):\n","          c = 5\n","        face = im[y-a:y+h+b, x-c:x+w+c]\n","        return (2, face)\n","  else:\n","  # Draw rectangles around the faces\n","    for (x, y, w, h) in faces:\n","      a, b, c = 0, 0, 0\n","      # Crop the face\n","      if(y-30 >= 0):\n","        a = 30\n","      if(y+h+10 < 218):\n","        b = 10\n","      if(x-5 >= 0 and x+w+5 < 178):\n","        c = 5\n","      face = im[y-a:y+h+b, x-c:x+w+c]\n","      return (2, face)"]},{"cell_type":"markdown","metadata":{"id":"EC644Ixypkz0"},"source":["We crop (by means of the function crop_face) the actor face."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYSJSDVfnk74"},"outputs":[],"source":["res = crop_face(actor_path, 1.12, 9)\n","if res[0] == 2:\n","  im = res[1]\n","else:\n","  print('Didn\\'t found a unique face for ' + actor_path[:-12])"]},{"cell_type":"markdown","metadata":{"id":"VZ4zsv0Ub_2B"},"source":["Now, we can add a custom background to our query image (i.e. the actor), in order to make it more realistic and less biased."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEf3IRQgc1C9"},"outputs":[],"source":["# First of all we make the background transparent\n","im = im.convert(\"RGBA\")\n","data = im.getdata()\n","newData = []\n","# Set alpha = 0 if the pixel is white (or almost white)\n","for item in data:\n","  if item[0] >= 240 and item[1] >= 240 and item[2] >= 240:\n","    newData.append((255, 255, 255, 0))\n","  else:\n","    newData.append(item)\n","im.putdata(newData)\n","# Now we paste the actor into the background\n","r = str((random.randint(0, 23))).zfill(2)\n","bg = Image.open('/content/gdrive/MyDrive/Digital Signal/Dataset/Backgrounds/' + r + '.jpg') \n","# Resize the background image according to the same actor image dimension\n","bg = bg.resize(im.size)\n","bg.paste(im, (0, 0), im)"]},{"cell_type":"markdown","source":["# Query"],"metadata":{"id":"U6St5USu0RUn"}},{"cell_type":"markdown","metadata":{"id":"_CjGCY3bvJsJ"},"source":["We query the tree that we previously loaded. Specifically, we take the first 2 nearest-neighbors for each actor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzbBDcLC0ZNo"},"outputs":[],"source":["def features_func(im):\n","  # Convert into numpy array\n","  x = kimage.img_to_array(im)\n","  # Preprocessing according to MobileNetV2\n","  x = keras.applications.mobilenet_v2.preprocess_input(x)\n","  # Expand dimensions\n","  x = np.expand_dims(x, axis = 0)\n","  # Extract features\n","  feat = mobilenet.predict(x, verbose = False)\n","  # Return features\n","  return feat.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZoIZ-9DuoCK"},"outputs":[],"source":["features_actors = features_func(bg)\n","features_actors = np.array(features_actors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTThyePOCyqK"},"outputs":[],"source":["dist, ind = tree.query(features_actors, k = 2)"]},{"cell_type":"markdown","metadata":{"id":"xahQGsxOHxNX"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DU0geIKRH6hD"},"outputs":[],"source":["# We don't consider the first column because is the id of the image\n","cols = df_actors.columns[1:]\n","\n","# Compute accuracy and distance for each actor\n","count_1 = 0\n","count_2 = 0\n","i = df_actors.index[df_actors['image_id'] == example].tolist()\n","# Save distances between actor and celebrity for both images\n","dist_1 = round(dist[0][0], 2)\n","dist_2 = round(dist[0][1], 2)\n","# Save indices related to the actor for both images\n","ind_1 = ind[0][0]\n","ind_2 = ind[0][1]\n","# Now we compute the matches between the binary attributes\n","for j in cols:\n","  # Compute matches for the first image\n","  if(df_actors[j][i] == df_celeba[j][ind_1]):\n","    count_1+=1\n","  # Compute matches for the second image\n","  if(df_actors[j][i] == df_celeba[j][ind_2]):\n","    count_2+=1\n","\n","# Compute accuracy for the first image\n","acc_1 = round((count_1/len(cols))*100, 2)\n","print('Accuracy for first image: ' + str(acc_1) + '%')\n","print('Distance for first image: ' + str(dist_1))\n","\n","# Compute accuracy for the second image\n","acc_2 = round((count_2/len(cols))*100, 2)\n","print('Accuracy for second image: ' + str(acc_2) + '%')\n","print('Distance for second image: ' + str(dist_2) + '\\n')"]},{"cell_type":"markdown","metadata":{"id":"nFUj2lEBM19s"},"source":["Displaying results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkkijTCYtuBv"},"outputs":[],"source":["im_celeb_1 = kimage.load_img('/content/gdrive/MyDrive/Digital Signal/Dataset/celeba/img_align_celeba' + str(ind[0][0]).zfill(6) + '.jpg', target_size = (224, 224))\n","im_celeb_2 = kimage.load_img('/content/gdrive/MyDrive/Digital Signal/Dataset/celeba/img_align_celeba' + str(ind[0][1]).zfill(6) + '.jpg', target_size = (224, 224))\n","\n","fig = plt.figure(figsize = (10, 7))\n","\n","fig.add_subplot(3, 1, 1)\n","plt.imshow(im)\n","plt.axis('off')\n","plt.title(example)\n","fig.add_subplot(3, 1, 2)\n","plt.imshow(im_celeb_1)\n","plt.axis('off')\n","plt.title(str(acc_1) + '%')\n","fig.add_subplot(3, 1, 3)\n","plt.imshow(im_celeb_2)\n","plt.axis('off')\n","plt.title(str(acc_2) + '%')"]},{"cell_type":"code","source":["# Print results\n","root = tk.Tk()\n","\n","# Root window title and dimension\n","root.title(\"Face retrieval\")\n","\n","canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n","canvas1.pack()\n","\n","# Display images with accuracy as caption, like in the cell above\n","'''\n","label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n","label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n","label3 = tk.Label(root, text=f'Global prediction:\\t{emotions[pred_global.argmax()]}')\n","label4 = tk.Label(root, text=f'Ground truth:\\t{emotions[label]}')\n","\n","label1.config(font=('helvetica', 14))\n","label2.config(font=('helvetica', 14))\n","label3.config(font=('helvetica', 14))\n","label4.config(font=('helvetica', 14), fg='gray')\n","\n","canvas1.create_window(20, 25, window=label1, anchor='w')\n","canvas1.create_window(20, 50, window=label2, anchor='w')\n","canvas1.create_window(20, 75, window=label3, anchor='w')\n","canvas1.create_window(20, 120, window=label4, anchor='w')\n","'''\n","\n","button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n","canvas1.create_window(200, 200, window=button1)\n","\n","root.mainloop()"],"metadata":{"id":"n8FzNFHo7w8s"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}