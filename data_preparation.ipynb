{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa0f050",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af0d7f",
   "metadata": {},
   "source": [
    "Features\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe300568",
   "metadata": {},
   "source": [
    "NB We considered only the speech videos (vocal channel=01) with both audio and video (modality=01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d260a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T17:27:37.510784Z",
     "start_time": "2023-01-01T17:27:37.486950Z"
    }
   },
   "outputs": [],
   "source": [
    "emotions = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}\n",
    "emotional_intensity = {1:'normal', 2:'strong'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bc976d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T17:27:13.200431Z",
     "start_time": "2023-01-01T17:27:10.028517Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d503e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T17:27:20.350634Z",
     "start_time": "2023-01-01T17:27:20.329824Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"Datasets/RAVDESS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526e8cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T17:27:27.666721Z",
     "start_time": "2023-01-01T17:27:27.647035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01-01-01-01-01-01-01',\n",
       " '01-01-01-01-01-02-01',\n",
       " '01-01-01-01-02-01-01',\n",
       " '01-01-01-01-02-02-01',\n",
       " '01-01-02-01-01-01-01']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = []\n",
    "feats = []\n",
    "labels = []\n",
    "paths = []\n",
    "\n",
    "for (dirpath, dirnames, fn) in os.walk(path):\n",
    "    for name in fn:\n",
    "        filename = name.split('.')[0]\n",
    "        feat = filename.split('-')[2:]\n",
    "        label = feat[0]\n",
    "        filenames.append(filename)\n",
    "        feats.append(feat)\n",
    "        labels.append(label)\n",
    "        paths.append(dirpath + '/' + filename)\n",
    "        \n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce701ade",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f05715f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T17:27:40.031915Z",
     "start_time": "2023-01-01T17:27:40.012195Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotional intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repetition</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01-01-01-01-01-01-01</th>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-01-01-01-02-01</th>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-01-01-02-01-01</th>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-01-01-02-02-01</th>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-02-01-01-01-01</th>\n",
       "      <td>calm</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-08-01-02-02-24</th>\n",
       "      <td>surprise</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-08-02-01-01-24</th>\n",
       "      <td>surprise</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-08-02-01-02-24</th>\n",
       "      <td>surprise</td>\n",
       "      <td>strong</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-08-02-02-01-24</th>\n",
       "      <td>surprise</td>\n",
       "      <td>strong</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01-01-08-02-02-02-24</th>\n",
       "      <td>surprise</td>\n",
       "      <td>strong</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       emotion emotional intensity  statement  repetition  \\\n",
       "index                                                                       \n",
       "01-01-01-01-01-01-01   neutral              normal          1           1   \n",
       "01-01-01-01-01-02-01   neutral              normal          1           2   \n",
       "01-01-01-01-02-01-01   neutral              normal          2           1   \n",
       "01-01-01-01-02-02-01   neutral              normal          2           2   \n",
       "01-01-02-01-01-01-01      calm              normal          1           1   \n",
       "...                        ...                 ...        ...         ...   \n",
       "01-01-08-01-02-02-24  surprise              normal          2           2   \n",
       "01-01-08-02-01-01-24  surprise              strong          1           1   \n",
       "01-01-08-02-01-02-24  surprise              strong          1           2   \n",
       "01-01-08-02-02-01-24  surprise              strong          2           1   \n",
       "01-01-08-02-02-02-24  surprise              strong          2           2   \n",
       "\n",
       "                      actor  \n",
       "index                        \n",
       "01-01-01-01-01-01-01      1  \n",
       "01-01-01-01-01-02-01      1  \n",
       "01-01-01-01-02-01-01      1  \n",
       "01-01-01-01-02-02-01      1  \n",
       "01-01-02-01-01-01-01      1  \n",
       "...                     ...  \n",
       "01-01-08-01-02-02-24     24  \n",
       "01-01-08-02-01-01-24     24  \n",
       "01-01-08-02-01-02-24     24  \n",
       "01-01-08-02-02-01-24     24  \n",
       "01-01-08-02-02-02-24     24  \n",
       "\n",
       "[1440 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(feats, columns = ['emotion', 'emotional intensity', 'statement', 'repetition', 'actor']).astype(int)\n",
    "\n",
    "df['emotion'] = df['emotion'].map(emotions)\n",
    "df['emotional intensity'] = df['emotional intensity'].map(emotional_intensity)\n",
    "\n",
    "df['index'] = filenames\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f7368",
   "metadata": {},
   "source": [
    "## Export frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ae3e3",
   "metadata": {},
   "source": [
    "- one frame every skip=3 starting from the 21th frame\n",
    "- proportional resize to obtain height=224\n",
    "- saved as png with and name videoname_iframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a032a1",
   "metadata": {},
   "source": [
    "### 398x224 normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955a9f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T18:28:26.762954Z",
     "start_time": "2023-01-01T18:28:26.754434Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_all_videos(filenames, paths, skip=1):\n",
    "    nframes_tot = 0\n",
    "    \n",
    "    for count, video in enumerate(zip(filenames, paths)):\n",
    "        # Gather all its frames\n",
    "        save_frames(video[0], video[1], video[1].replace('RAVDESS', 'RAVDESS_frames'), skip)\n",
    "        print(f\"Processed videos {count+1}/{len(paths)}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def save_frames(filename, input_path, output_path, skip):\n",
    "    # Initialize video reader\n",
    "    cap = cv2.VideoCapture(input_path + '.mp4')\n",
    "    frames = []\n",
    "    count = 0\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    try:\n",
    "        # Loop through all frames\n",
    "        while True:\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if (count % skip == 0 and count > 20):\n",
    "                #print(frame.shape)\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (398, 224))\n",
    "                cv2.imwrite(output_path + '/' + f'{filename}_{count}' + '.png', frame)\n",
    "            count += 1\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c48ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T18:48:13.241324Z",
     "start_time": "2023-01-01T18:34:00.912543Z"
    }
   },
   "outputs": [],
   "source": [
    "prepare_all_videos(filenames, paths, skip=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a39bcd8f",
   "metadata": {},
   "source": [
    "### 224x224 black background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(filenames, paths, skip=1):\n",
    "    nframes_tot = 0\n",
    "    \n",
    "    for count, video in enumerate(zip(filenames, paths)):\n",
    "        # Gather all its frames\n",
    "        save_frames(video[0], video[1], video[1].replace('RAVDESS', 'RAVDESS_frames_black'), skip)\n",
    "        print(f\"Processed videos {count+1}/{len(paths)}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def save_frames(filename, input_path, output_path, skip):\n",
    "    # Initialize video reader\n",
    "    cap = cv2.VideoCapture(input_path + '.mp4')\n",
    "    frames = []\n",
    "    count = 0\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    try:\n",
    "        # Loop through all frames\n",
    "        while True:\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if (count % skip == 0 and count > 20):\n",
    "                #print(frame.shape)\n",
    "                if not ret:\n",
    "                    break\n",
    "                #####\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)                  # background from white to black\n",
    "                ret, thresh = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY)\n",
    "                frame[thresh == 255] = 0\n",
    "                #####\n",
    "                frame = cv2.resize(frame, (398, 224))\n",
    "                frame = frame[0:224, 87:311]\n",
    "                cv2.imwrite(output_path + '/' + f'{filename}_{count}' + '.png', frame)\n",
    "            count += 1\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_all_videos(filenames, paths, skip=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b84b0c65",
   "metadata": {},
   "source": [
    "### 224x224 only faces BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a08142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(filenames, paths, skip=1):\n",
    "    nframes_tot = 0\n",
    "    \n",
    "    for count, video in enumerate(zip(filenames, paths)):\n",
    "        # Gather all its frames\n",
    "        save_frames(video[0], video[1], video[1].replace('RAVDESS', 'RAVDESS_frames_face_BW'), skip)\n",
    "        print(f\"Processed videos {count+1}/{len(paths)}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def save_frames(filename, input_path, output_path, skip):\n",
    "    # Initialize video reader\n",
    "    cap = cv2.VideoCapture(input_path + '.mp4')\n",
    "    haar_cascade = cv2.CascadeClassifier('./Other/haarcascade_frontalface_default.xml')\n",
    "    frames = []\n",
    "    count = 0\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    try:\n",
    "        # Loop through all frames\n",
    "        while True:\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if (count % skip == 0 and count > 20):\n",
    "                #print(frame.shape)\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.12, minNeighbors=9)\n",
    "                # if len(faces) != 1:\n",
    "                    \n",
    "                if len(faces) == 0:\n",
    "                    faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.02, minNeighbors=9)\n",
    "                    if len(faces) == 0:\n",
    "                        raise Exception(f\"Still no faces {len(faces)} {filename}\")\n",
    "                if len(faces) > 1:\n",
    "                    ex = []\n",
    "                    print(type(faces))\n",
    "                    for elem in faces:\n",
    "                        for (x, y, w, h) in [elem]:\n",
    "                            ex.append(frame[y:y + h, x:x + w])\n",
    "\n",
    "                    print(filename)\n",
    "                    plt.figure()\n",
    "                    f, axarr = plt.subplots(4,1)\n",
    "                    axarr[0].imshow(ex[0])\n",
    "                    axarr[1].imshow(ex[1])\n",
    "                    plt.show()\n",
    "\n",
    "                    inp = int(input())\n",
    "                    faces = [faces[inp]]\n",
    "                #     raise Exception(f\"More than 1 faces detected in {filename}\")\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    face = frame[y:y + h, x:x + w]\n",
    "\n",
    "                face = cv2.resize(face, (234, 234))\n",
    "                face = face[5:-5, 5:-5]\n",
    "                cv2.imwrite(output_path + '/' + f'{filename}_{count}' + '.png', face)\n",
    "            count += 1\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_all_videos(filenames, paths, skip=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40c6b19c",
   "metadata": {},
   "source": [
    "### PCA 50 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08ca70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_tras = {1:1, 2:4, 3:5, 4:0, 5:3, 6:2, 7:6}\n",
    "emotions = {0:'angry', 1:'calm', 2:'disgust', 3:'fear', 4:'happy', 5:'sad', 6:'surprise'}\n",
    "\n",
    "# emotions = {0:'neutral', 1:'calm', 2:'happy', 3:'sad', 4:'angry', 5:'fear', 6:'disgust', 7:'surprise'}\n",
    "# emotional_intensity = {1:'normal', 2:'strong'}\n",
    "\n",
    "dataset_path = \"Datasets/RAVDESS_frames_face_BW/\"\n",
    "output_path = \"Datasets/RAVDESS_frames_face_BW_SVD50/\"\n",
    "\n",
    "n_components = 50\n",
    "\n",
    "height_orig = 224\n",
    "width_orig = 224\n",
    "height_targ = 112\n",
    "width_targ = 112\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = len(emotions)\n",
    "\n",
    "val_actors = ['21', '22']\n",
    "test_actors = ['23', '24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_train = [] # train\n",
    "filenames_val = [] # validation\n",
    "\n",
    "for (dirpath, dirnames, fn) in os.walk(dataset_path):\n",
    "    if fn != []:\n",
    "        class_temp = int(fn[0].split('-')[2]) - 1\n",
    "        if class_temp != 0:                                                     # exclude 'neutral' label\n",
    "            if any(act in dirpath for act in (test_actors+val_actors))==False:  # select only train actors\n",
    "                path = [os.path.join(dirpath, elem) for elem in fn]\n",
    "                label = [emotions_tras[class_temp]] * len(fn)                   # emotion transposition\n",
    "                filenames_train.append(list(zip(path, label)))\n",
    "            \n",
    "            if any(act in dirpath for act in val_actors):                       # select only validation actors\n",
    "                path = [os.path.join(dirpath, elem) for elem in fn]\n",
    "                label = [emotions_tras[class_temp]] * len(fn)\n",
    "                filenames_val.append(list(zip(path, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474408f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(list, num_frames_desired):\n",
    "    tot = []\n",
    "    for elem in list:\n",
    "        sampled_list = random.sample(elem, num_frames_desired)\n",
    "        tot += sampled_list\n",
    "    return(tot)\n",
    "\n",
    "\n",
    "def load_dataset(filenames, n_components):\n",
    "    frames_per_vid = min([len(elem) for elem in filenames])     # number of frames per clip in order to have balanced classes\n",
    "    print(\"frames per video:\", frames_per_vid) \n",
    "\n",
    "    filenames_sampled = sampling(filenames, frames_per_vid)\n",
    "    random.shuffle(filenames_sampled)\n",
    "\n",
    "    faces = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    for path, label in tqdm(filenames_sampled):\n",
    "        face = cv2.imread(path)\n",
    "        face = cv2.resize(face, (width_targ, height_targ))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces.append(face)\n",
    "        y.append(label)\n",
    "    \n",
    "    # plt.imshow(x[1], cmap='gray')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    faces = np.array(faces)\n",
    "\n",
    "    avgFace = np.average(faces, axis=0)\n",
    "    print('mean of faces')\n",
    "    plt.imshow(avgFace, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(faces.shape)\n",
    "    mean = np.mean(faces, axis=0)\n",
    "    faces = [face - mean for face in faces]\n",
    "\n",
    "    print('face post')\n",
    "    plt.imshow(faces[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    faces = np.array(faces)\n",
    "    # faces = np.reshape(faces, (100, ))\n",
    "    faces = faces.reshape((faces.shape[0], faces.shape[1]**2))\n",
    "    print(faces.shape)\n",
    "\n",
    "    #now calculate eigen values and eigen vectors for cov_mat\n",
    "    # cov_mat = np.cov(faces, rowvar = False)\n",
    "    # print('computer covariace, now eig')\n",
    "\n",
    "    # eig_values, eig_vectors = np.linalg.eigh(cov_mat)\n",
    "    # print('computed eig, now sorting and cropping')\n",
    "\n",
    "    # #sort the eigenvalues in descending order\n",
    "    # sorted_index = np.argsort(eig_values)[::-1]\n",
    "    \n",
    "    # sorted_eigenvalue = eig_values[sorted_index]\n",
    "    # #similarly sort the eigenvectors \n",
    "    # sorted_eigenvectors = eig_vectors[:,sorted_index]\n",
    "\n",
    "    \n",
    "    # eigenvector_subset = sorted_eigenvectors[:,0:n_components]\n",
    "    # print(eigenvector_subset.shape)\n",
    "    pca = PCA(n_components) # we need 2 principal components.\n",
    "    x_reduced = pca.fit_transform(faces)\n",
    "    \n",
    "    # converted_data.shape\n",
    "\n",
    "\n",
    "    # x_reduced = []\n",
    "    # for face in faces:\n",
    "\n",
    "    #     x_reduced = np.dot(eigenvector_subset.transpose(),face.transpose()).transpose()\n",
    "    #     print(x_reduced.shape)\n",
    "\n",
    "    # names = tf.data.Dataset.from_tensor_slices(names)\n",
    "    # images = names.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # labels = [elem for elem in labels]\n",
    "    # labels = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\n",
    "    # ds = tf.data.Dataset.zip((images, labels))\n",
    "    # ds = configure_for_performance(ds)\n",
    "\n",
    "    frame_number = len(filenames_sampled)\n",
    "    step_per_epoch = frame_number // batch_size\n",
    "    print('frames number:', frame_number, '\\nbatch size:', batch_size, '\\nbatch number:', step_per_epoch)\n",
    "    return x_reduced, y, step_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, y, step_per_epoch_train = load_dataset(filenames_train[:80], n_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
