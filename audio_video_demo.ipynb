{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Emotion Recognition: audio + video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Audio and video manipulation\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import load\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels dictionary\n",
    "emotions_tras = {1:1, 2:4, 3:5, 4:0, 5:3, 6:2, 7:6}\n",
    "emotions = {0:'angry', 1:'calm', 2:'disgust', 3:'fear', 4:'happy', 5:'sad', 6:'surprise'}\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"Datasets/Demo/\"\n",
    "parameters_path = 'Datasets/Audio_Speech/std_scaler.bin'\n",
    "models_video_path = \"Models/Video_stream/\"\n",
    "models_audio_path = \"Models/Audio_stream/\"\n",
    "vlc_path = \"C:/Program Files/VideoLAN/VLC/vlc.exe\"\n",
    "\n",
    "# Audio video parameters\n",
    "height_targ = 112\n",
    "width_targ = 112\n",
    "sr = 48000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Select clip to analize')\n",
    "label1.config(font=('helvetica', 16))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "\n",
    "label2 = tk.Label(root, text='Number from 0 to 6:')\n",
    "label2.config(font=('helvetica', 11))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "def display_text():\n",
    "   global example\n",
    "   example = int(example.get())\n",
    "   root.destroy\n",
    "\n",
    "example = tk.Entry(root)\n",
    "example.pack()\n",
    "canvas1.create_window(200, 140, window=example)\n",
    "\n",
    "    \n",
    "button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.listdir(dataset_path)\n",
    "filename = dataset_path + fn[example]\n",
    "label = emotions_tras[int(fn[example].split('-')[2]) - 1] # trasposition of the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = subprocess.call([vlc_path, filename, '--play-and-exit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4a2239",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape frames: (30, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(filename)\n",
    "haar_cascade = cv2.CascadeClassifier('./Other/haarcascade_frontalface_default.xml')\n",
    "frames = []\n",
    "count = 0\n",
    "skip = 3\n",
    "\n",
    "# Loop through all frames\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if (count % skip == 0 and count > 20):\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and crop face\n",
    "        faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.12, minNeighbors=9)\n",
    "        if len(faces) != 1:\n",
    "            continue\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        face = cv2.resize(face, (height_targ+10, width_targ+10))\n",
    "        face = face[5:-5, 5:-5]\n",
    "        face = face/255.\n",
    "        frames.append(face)\n",
    "    count += 1\n",
    "\n",
    "frames = np.array(frames)\n",
    "num_frames = len(frames)\n",
    "labels = [label] * num_frames\n",
    "print('shape frames:', frames.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = mp.AudioFileClip(filename).set_fps(sr)\n",
    "audio = audiofile.to_soundarray()\n",
    "audio = audio[int(sr/2):int(sr/2 + sr*3)]\n",
    "audio = np.array([elem[0] for elem in audio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 282, 1)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = librosa.power_to_db(librosa.feature.melspectrogram(audio, sr = 48000, n_fft = 1024, n_mels = 128, fmin = 50, fmax = 24000)) \n",
    "\n",
    "scaler = load(parameters_path)\n",
    "mel = scaler.transform(mel)\n",
    "\n",
    "mel = np.expand_dims(mel, axis = 2)\n",
    "mel = np.expand_dims(mel, axis = 0)\n",
    "mel.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_video_path)\n",
    "\n",
    "acc = [float(model.split('[')[1].split(']')[0]) for model in models_list]\n",
    "idx = acc.index(max(acc))                                                       # index of best model\n",
    "\n",
    "model_video = keras.models.load_model(models_video_path + models_list[idx])\n",
    "# model_video.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_audio_path)\n",
    "model_audio = keras.models.load_model(models_audio_path + models_list[0])\n",
    "# model_audio.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.0242448e-01, 3.6509120e-04, 1.1092870e-02, 2.5384623e-01,\n",
       "       1.1387592e-02, 1.8433736e-03, 5.1904041e-01], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_video.predict(frames)\n",
    "pred_video = np.mean(pred, axis=0)\n",
    "pred_video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.2988043e-04, 1.5817456e-08, 1.7952883e-04, 5.9027163e-05,\n",
       "       1.4042200e-03, 1.6080631e-06, 9.9792576e-01], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_audio.predict(mel)\n",
    "pred_audio = np.mean(pred, axis=0)\n",
    "pred_audio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_global = pred_video + pred_audio # mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video prediction:\t surprise\n",
      "Audio prediction:\t surprise\n",
      "Global prediction:\t surprise\n",
      "Ground truth:\t\t surprise\n"
     ]
    }
   ],
   "source": [
    "print('Video prediction:\\t', emotions[pred_video.argmax()])\n",
    "print('Audio prediction:\\t', emotions[pred_audio.argmax()])\n",
    "print('Global prediction:\\t', emotions[pred_global.argmax()])\n",
    "\n",
    "print('Ground truth:\\t\\t', emotions[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "root = tk.Tk()\n",
    "\n",
    "# root window title and dimension\n",
    "root.title(\"Results predictions\")\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n",
    "label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n",
    "label3 = tk.Label(root, text=f'Global prediction:\\t{emotions[pred_global.argmax()]}')\n",
    "label4 = tk.Label(root, text=f'Ground truth:\\t{emotions[label]}')\n",
    "\n",
    "label1.config(font=('helvetica', 14))\n",
    "label2.config(font=('helvetica', 14))\n",
    "label3.config(font=('helvetica', 14))\n",
    "label4.config(font=('helvetica', 14), fg='gray')\n",
    "\n",
    "canvas1.create_window(20, 25, window=label1, anchor='w')\n",
    "canvas1.create_window(20, 50, window=label2, anchor='w')\n",
    "canvas1.create_window(20, 75, window=label3, anchor='w')\n",
    "canvas1.create_window(20, 120, window=label4, anchor='w')\n",
    "\n",
    "button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 200, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
