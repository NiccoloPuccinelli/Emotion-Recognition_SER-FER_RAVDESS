{"cells":[{"cell_type":"markdown","metadata":{"id":"ytB37MWwq5K4"},"source":["# Preliminary operations"]},{"cell_type":"markdown","metadata":{"id":"5Wbyemljr_qB"},"source":["We import libraries and datasets."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1675287438200,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"yu3Mcu_Tq23l"},"outputs":[],"source":["# Utility\n","# from google.colab import drive\n","# from shutil import copyfile\n","import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import numpy as np\n","import tkinter as tk\n","import random\n","\n","# Image processing\n","import cv2\n","from PIL import Image\n","\n","# KDTree\n","from sklearn.neighbors import KDTree\n","import joblib\n","# import sklearn\n","\n","# Keras\n","# from tensorflow import keras\n","from keras.preprocessing import image as kimage\n","from keras.applications import mobilenet_v2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29606,"status":"ok","timestamp":1675287009872,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"B6FZ9uhfybKF","outputId":"a891d3f5-051e-4926-f00b-18304004380d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["# drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"7_vUMlK13-JK"},"source":["Loading the tree."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":24801,"status":"ok","timestamp":1675287037828,"user":{"displayName":"Niccolò Puccinelli","userId":"16598033676719318077"},"user_tz":-60},"id":"yvy_1JdTg8Bx"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn.metrics._dist_metrics'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22484/3579180708.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# tree = joblib.load('/content/gdrive/MyDrive/Digital Signal/Models/Image Retrieval/mob_tree.joblib')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Models/Image Retrieval/mob_tree.joblib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    656\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n","\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1538\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1577\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.metrics._dist_metrics'"]}],"source":["# tree = joblib.load('/content/gdrive/MyDrive/Digital Signal/Models/Image Retrieval/mob_tree.joblib')\n","tree = joblib.load('Models/Face Image Retrieval/mob_tree.joblib')"]},{"cell_type":"markdown","metadata":{"id":"hOfUPR_i68yS"},"source":["Loading the MobileNetV2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJ3r4UFVrsnN"},"outputs":[],"source":["mobilenet = keras.applications.MobileNetV2(input_shape = (224, 224, 3), weights = 'imagenet', include_top = False, pooling = 'max')"]},{"cell_type":"markdown","metadata":{"id":"n96rMlZN7JQd"},"source":["Loading the dataframes for computing accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8IP2P6HKxxu"},"outputs":[],"source":["# df_actors = pd.read_csv('/content/gdrive/MyDrive/Digital Signal/Dataset/list_attr_actors.csv')\n","# df_celeba = pd.read_csv('/content/gdrive/MyDrive/Digital Signal/Dataset/list_attr_celeba.csv')\n","\n","df_actors = pd.read_csv('Datasets/CelebA/list_attr_actors.csv')\n","df_celeba = pd.read_csv('Datasets/CelebA/list_attr_celeba.csv')"]},{"cell_type":"markdown","metadata":{"id":"L3EwsRlA2vaa"},"source":["# Acquisition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZ_caZGo2uY2"},"outputs":[],"source":["root= tk.Tk()\n","\n","canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n","canvas1.pack()\n","\n","label1 = tk.Label(root, text='Select clip to analize')\n","label1.config(font=('helvetica', 16))\n","canvas1.create_window(200, 25, window=label1)\n","\n","label2 = tk.Label(root, text='Number from 0 to 6:')\n","label2.config(font=('helvetica', 11))\n","canvas1.create_window(200, 100, window=label2)\n","\n","def display_text():\n","   global example\n","   example = int(example.get())\n","   root.destroy\n","\n","example = tk.Entry(root)\n","example.pack()\n","canvas1.create_window(200, 140, window=example)\n","\n","    \n","button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n","canvas1.create_window(200, 180, window=button1)\n","\n","root.mainloop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ls9iaqU94-Hm"},"outputs":[],"source":["# fn = os.listdir('/content/gdrive/MyDrive/Digital Signal/Dataset/Actor_faces/')\n","# actor_path = '/content/gdrive/MyDrive/Digital Signal/Dataset/Actor_faces/' + fn[example]\n","\n","fn = os.listdir('Datasets/Demo/Image Retrieval/')\n","actor_path = 'Datasets/Demo/Image Retrieval/' + fn[example]"]},{"cell_type":"markdown","metadata":{"id":"c4Gqu4tw2uoE"},"source":["# Processing"]},{"cell_type":"markdown","metadata":{"id":"-WP0oaTebgpW"},"source":["In order to crop the background as much as possible, we use a Haar cascade classifier to detect and crop faces in both datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJ6G5veEb-1s"},"outputs":[],"source":["# Load the cascade classifier\n","face_cascade = cv2.CascadeClassifier('/content/gdrive/MyDrive/Digital Signal/Dataset/haarcascade_frontalface_default.xml')\n","\n","def crop_face(path, scaleFactor, minNeighbors):\n","  # Read the input image\n","  im = cv2.imread(path)\n","  # Convert the image to grayscale\n","  gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n","  # Detect faces in the image\n","  faces = face_cascade.detectMultiScale(gray, scaleFactor, minNeighbors)\n","  if len(faces) == 0:\n","    # We may try to decrease the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor - 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        a, b, c = 0, 0, 0\n","        # Crop the face\n","        if(y-30 >= 0):\n","          a = 30\n","        if(y+h+10 < 218):\n","          b = 10\n","        if(x-5 >= 0 and x+w+5 < 178):\n","          c = 5\n","        face = im[y-a:y+h+b, x-c:x+w+c]\n","        return (2, face)\n","  elif len(faces) > 1:\n","    # We may try to increase the scaleFactor parameter\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor + 0.1, minNeighbors)\n","    if len(faces) == 0:\n","      return (0, 0)\n","    elif len(faces) > 1:\n","      return (1, 0)\n","    else:\n","      # Draw rectangles around the faces\n","      for (x, y, w, h) in faces:\n","        a, b, c = 0, 0, 0\n","        # Crop the face\n","        if(y-30 >= 0):\n","          a = 30\n","        if(y+h+10 < 218):\n","          b = 10\n","        if(x-5 >= 0 and x+w+5 < 178):\n","          c = 5\n","        face = im[y-a:y+h+b, x-c:x+w+c]\n","        return (2, face)\n","  else:\n","  # Draw rectangles around the faces\n","    for (x, y, w, h) in faces:\n","      a, b, c = 0, 0, 0\n","      # Crop the face\n","      if(y-30 >= 0):\n","        a = 30\n","      if(y+h+10 < 218):\n","        b = 10\n","      if(x-5 >= 0 and x+w+5 < 178):\n","        c = 5\n","      face = im[y-a:y+h+b, x-c:x+w+c]\n","      return (2, face)"]},{"cell_type":"markdown","metadata":{"id":"EC644Ixypkz0"},"source":["We crop (by means of the function crop_face) the actor face."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYSJSDVfnk74"},"outputs":[],"source":["res = crop_face(actor_path, 1.12, 9)\n","if res[0] == 2:\n","  im = res[1]\n","else:\n","  print('Didn\\'t found a unique face for ' + actor_path[:-12])"]},{"cell_type":"markdown","metadata":{"id":"VZ4zsv0Ub_2B"},"source":["Now, we can add a custom background to our query image (i.e. the actor), in order to make it more realistic and less biased."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEf3IRQgc1C9"},"outputs":[],"source":["# First of all we make the background transparent\n","im = im.convert(\"RGBA\")\n","data = im.getdata()\n","newData = []\n","# Set alpha = 0 if the pixel is white (or almost white)\n","for item in data:\n","  if item[0] >= 240 and item[1] >= 240 and item[2] >= 240:\n","    newData.append((255, 255, 255, 0))\n","  else:\n","    newData.append(item)\n","im.putdata(newData)\n","# Now we paste the actor into the background\n","r = str((random.randint(0, 23))).zfill(2)\n","bg = Image.open('/content/gdrive/MyDrive/Digital Signal/Dataset/Backgrounds/' + r + '.jpg') \n","# Resize the background image according to the same actor image dimension\n","bg = bg.resize(im.size)\n","bg.paste(im, (0, 0), im)"]},{"cell_type":"markdown","metadata":{"id":"U6St5USu0RUn"},"source":["# Query"]},{"cell_type":"markdown","metadata":{"id":"_CjGCY3bvJsJ"},"source":["We query the tree that we previously loaded. Specifically, we take the first 2 nearest-neighbors for each actor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzbBDcLC0ZNo"},"outputs":[],"source":["def features_func(im):\n","  # Convert into numpy array\n","  x = kimage.img_to_array(im)\n","  # Preprocessing according to MobileNetV2\n","  x = keras.applications.mobilenet_v2.preprocess_input(x)\n","  # Expand dimensions\n","  x = np.expand_dims(x, axis = 0)\n","  # Extract features\n","  feat = mobilenet.predict(x, verbose = False)\n","  # Return features\n","  return feat.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZoIZ-9DuoCK"},"outputs":[],"source":["features_actors = features_func(bg)\n","features_actors = np.array(features_actors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTThyePOCyqK"},"outputs":[],"source":["dist, ind = tree.query(features_actors, k = 2)"]},{"cell_type":"markdown","metadata":{"id":"xahQGsxOHxNX"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DU0geIKRH6hD"},"outputs":[],"source":["# We don't consider the first column because is the id of the image\n","cols = df_actors.columns[1:]\n","\n","# Compute accuracy and distance for each actor\n","count_1 = 0\n","count_2 = 0\n","i = df_actors.index[df_actors['image_id'] == example].tolist()\n","# Save distances between actor and celebrity for both images\n","dist_1 = round(dist[0][0], 2)\n","dist_2 = round(dist[0][1], 2)\n","# Save indices related to the actor for both images\n","ind_1 = ind[0][0]\n","ind_2 = ind[0][1]\n","# Now we compute the matches between the binary attributes\n","for j in cols:\n","  # Compute matches for the first image\n","  if(df_actors[j][i] == df_celeba[j][ind_1]):\n","    count_1+=1\n","  # Compute matches for the second image\n","  if(df_actors[j][i] == df_celeba[j][ind_2]):\n","    count_2+=1\n","\n","# Compute accuracy for the first image\n","acc_1 = round((count_1/len(cols))*100, 2)\n","print('Accuracy for first image: ' + str(acc_1) + '%')\n","print('Distance for first image: ' + str(dist_1))\n","\n","# Compute accuracy for the second image\n","acc_2 = round((count_2/len(cols))*100, 2)\n","print('Accuracy for second image: ' + str(acc_2) + '%')\n","print('Distance for second image: ' + str(dist_2) + '\\n')"]},{"cell_type":"markdown","metadata":{"id":"nFUj2lEBM19s"},"source":["Displaying results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkkijTCYtuBv"},"outputs":[],"source":["im_celeb_1 = kimage.load_img('/content/gdrive/MyDrive/Digital Signal/Dataset/celeba/img_align_celeba' + str(ind[0][0]).zfill(6) + '.jpg', target_size = (224, 224))\n","im_celeb_2 = kimage.load_img('/content/gdrive/MyDrive/Digital Signal/Dataset/celeba/img_align_celeba' + str(ind[0][1]).zfill(6) + '.jpg', target_size = (224, 224))\n","\n","fig = plt.figure(figsize = (10, 7))\n","\n","fig.add_subplot(3, 1, 1)\n","plt.imshow(im)\n","plt.axis('off')\n","plt.title(example)\n","fig.add_subplot(3, 1, 2)\n","plt.imshow(im_celeb_1)\n","plt.axis('off')\n","plt.title(str(acc_1) + '%')\n","fig.add_subplot(3, 1, 3)\n","plt.imshow(im_celeb_2)\n","plt.axis('off')\n","plt.title(str(acc_2) + '%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8FzNFHo7w8s"},"outputs":[],"source":["# Print results\n","root = tk.Tk()\n","\n","# Root window title and dimension\n","root.title(\"Face retrieval\")\n","\n","canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n","canvas1.pack()\n","\n","# Display images with accuracy as caption, like in the cell above\n","'''\n","label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n","label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n","label3 = tk.Label(root, text=f'Global prediction:\\t{emotions[pred_global.argmax()]}')\n","label4 = tk.Label(root, text=f'Ground truth:\\t{emotions[label]}')\n","\n","label1.config(font=('helvetica', 14))\n","label2.config(font=('helvetica', 14))\n","label3.config(font=('helvetica', 14))\n","label4.config(font=('helvetica', 14), fg='gray')\n","\n","canvas1.create_window(20, 25, window=label1, anchor='w')\n","canvas1.create_window(20, 50, window=label2, anchor='w')\n","canvas1.create_window(20, 75, window=label3, anchor='w')\n","canvas1.create_window(20, 120, window=label4, anchor='w')\n","'''\n","\n","button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n","canvas1.create_window(200, 200, window=button1)\n","\n","root.mainloop()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
